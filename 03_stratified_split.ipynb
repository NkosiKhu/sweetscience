{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a10ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import av  # pip install av\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import evaluate  # pip install evaluate\n",
    "\n",
    "# load environment variables with dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5398097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5df0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Point this at the Olympic Boxing dataset directory\n",
    "\n",
    "# Pretrained VideoMAE base (self-supervised on K400)\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL2ID),\n",
    "    label2id=LABEL2ID,\n",
    "    id2label=ID2LABEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79ab45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14677eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_data():\n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "    test_paths = []\n",
    "\n",
    "    # Collect all paths with their labels and sources\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    all_sources = []\n",
    "\n",
    "    for label in os.listdir(\"preprocessed_clips_2\"):\n",
    "        paths = [f\"preprocessed_clips_2/{label}/{p}\" for p in os.listdir(f\"preprocessed_clips_2/{label}\")]\n",
    "        \n",
    "        for path in paths:\n",
    "            # Extract source from filename: pattern is clip_task_[kamx_nums]_index_c.npy\n",
    "            # Source is the part matching task_[kamx_nums]_index_c (everything from task_ to .npy)\n",
    "            filename = os.path.basename(path)\n",
    "            # Match task_ followed by any characters until .npy\n",
    "            source_match = re.search(r'task_kam\\d+_[^_]+', filename)\n",
    "            if source_match:\n",
    "                source = source_match.group(0)\n",
    "            else:\n",
    "                # Fallback: use filename without extension as source\n",
    "                source = os.path.splitext(filename)[0]\n",
    "            \n",
    "            all_paths.append(path)\n",
    "            all_labels.append(label)\n",
    "            all_sources.append(source)\n",
    "\n",
    "    # Create combined stratification key: label_source\n",
    "    # This ensures both label and source distributions are maintained\n",
    "    stratify_key = [f\"{label}_{source}\" for label, source in zip(all_labels, all_sources)]\n",
    "\n",
    "\n",
    "    # First split: 80% train, 20% temp (which will become val+test)\n",
    "    train_paths, temp_paths, train_labels, temp_labels, train_sources, temp_sources = train_test_split(\n",
    "        all_paths, all_labels, all_sources,\n",
    "        test_size=0.2,\n",
    "        stratify=stratify_key,\n",
    "        random_state=632\n",
    "    )\n",
    "\n",
    "    # Second split: split temp into 50% val, 50% test (which gives 10% val, 10% test overall)\n",
    "    # Create new stratification key for temp split\n",
    "    temp_stratify_key = [f\"{label}_{source}\" for label, source in zip(temp_labels, temp_sources)]\n",
    "\n",
    "    temp_counts = Counter(temp_stratify_key)\n",
    "    min_count = min(temp_counts.values())\n",
    "\n",
    "    if min_count >= 2:\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=temp_stratify_key,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # Fall back to non-stratified split if some classes have < 2 members\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=None,\n",
    "            random_state=632\n",
    "        )\n",
    "\n",
    "    # Convert to class variables\n",
    "    train_paths = train_paths\n",
    "    val_paths = val_paths\n",
    "    test_paths = test_paths\n",
    "    \n",
    "    return train_paths, val_paths, test_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ede674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import random\n",
    "class BoxingDataset(Dataset):\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    all_splits = split_data()\n",
    "    train_paths = all_splits[0]\n",
    "    val_paths = all_splits[1]\n",
    "    test_paths = all_splits[2]\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self, split: str):\n",
    "        self.split = split\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.train_paths)\n",
    "        elif self.split == \"val\":\n",
    "            return len(self.val_paths)\n",
    "        elif self.split == \"test\":\n",
    "            return len(self.test_paths)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            path = self.train_paths[idx]\n",
    "        elif self.split == \"val\":\n",
    "            path = self.val_paths[idx]\n",
    "        elif self.split == \"test\":\n",
    "            path = self.test_paths[idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "        \n",
    "        clip = np.load(path)\n",
    "        \n",
    "        # convert to float and scale to 0-1\n",
    "        clip = clip.astype(np.float32) / 255.0\n",
    "        \n",
    "        # image net mean/std\n",
    "        clip = (clip - self.mean) / self.std\n",
    "        \n",
    "        #reorder to (T,C,H,W)\n",
    "        clip = clip.transpose(0,3,1,2)\n",
    "        \n",
    "        #convert to tensor\n",
    "        clip = torch.from_numpy(clip)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": clip,\n",
    "            \"labels\": torch.tensor(LABEL2ID[path.split(\"/\")[-2]], dtype=torch.long) \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b171cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BoxingDataset(\n",
    "    split=\"train\",\n",
    ")\n",
    "val_dataset = BoxingDataset(\n",
    "    split=\"val\",\n",
    ")\n",
    "test_dataset = BoxingDataset(\n",
    "    split=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a143d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FACTS used batch_size=4, grad_accum=2, warmup_ratio=0.1, epochs=10\n",
    "# Learning rate is not rendered in the HTML; start with 1e-4 and tune around it.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./facts-boxing-videomae\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=8, \n",
    "    gradient_accumulation_steps=2,  # effective batch size 8\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.05,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"wandb\",  # or \"wandb\"/\"tensorboard\"\n",
    "    dataloader_num_workers=4,        # ADD THIS - use multiple workers\n",
    "    dataloader_pin_memory=True,      # ADD THIS - faster CPU->GPU transfer\n",
    "    dataloader_prefetch_factor=2, \n",
    ")\n",
    "\n",
    "data_collator = VideoDataCollator()\n",
    "\n",
    "train_labels = [LABEL2ID[path.split(\"/\")[-2]] for path in BoxingDataset.train_paths]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(LABEL2ID)),\n",
    "    y=np.array(train_labels)  # Ensure it's a numpy array\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a544dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x77e57550f760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    sample_weights,                                              \n",
    "    len(sample_weights), \n",
    "    replacement=True\n",
    ")\n",
    "sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0d03a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIThJREFUeJzt3X9wFPX9x/HXhSM/ivlBYpPLlQRSSwUFIhKJEfotSsYYkMJIVZxII1CoNlEgrWJmBCxVAgxqBCMRawGnpKidEpWOoTFoqGMIkJSOPxiEGiUDXtIOJiFxCJHb7x+d3sxJtAb33E/C8zGzM97u3ua9FfXZvd2cy7IsSwAAAAYJc3oAAACALyJQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABjH7fQAF8Lv9+vkyZOKjo6Wy+VyehwAAPA1WJal06dPy+v1Kizsq6+R9MtAOXnypFJSUpweAwAAXIDm5mYNGzbsK/fpl4ESHR0t6T8nGBMT4/A0AADg6+jo6FBKSkrgv+NfpV8Gyn8/1omJiSFQAADoZ77O7RncJAsAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIzT50DZu3evZsyYIa/XK5fLpcrKyi/d9+6775bL5VJpaWnQ+lOnTikvL08xMTGKi4vTggUL1NnZ2ddRAADAAOXu6xu6urqUnp6u+fPn65ZbbvnS/Xbu3Kl9+/bJ6/Wety0vL0+ffPKJqqur1dPTo3nz5mnRokWqqKjo6zgAMOCMePAvTo9wQT5aM93pETCA9DlQcnNzlZub+5X7nDhxQvfee692796t6dOD/8AePnxYVVVVOnDggDIyMiRJGzdu1LRp07R+/fpegwYAAFxcbL8Hxe/3a+7cubr//vt15ZVXnre9rq5OcXFxgTiRpOzsbIWFham+vr7XY3Z3d6ujoyNoAQAAA5ftgbJ27Vq53W7dd999vW73+XxKTEwMWud2uxUfHy+fz9fre0pKShQbGxtYUlJS7B4bAAAYxNZAaWho0JNPPqmtW7fK5XLZdtzi4mK1t7cHlubmZtuODQAAzGNroPztb39Ta2urUlNT5Xa75Xa79fHHH+tXv/qVRowYIUnyeDxqbW0Net/nn3+uU6dOyePx9HrciIgIxcTEBC0AAGDg6vNNsl9l7ty5ys7ODlqXk5OjuXPnat68eZKkrKwstbW1qaGhQRMmTJAk7dmzR36/X5mZmXaOAwAA+qk+B0pnZ6eOHTsWeN3U1KRDhw4pPj5eqampSkhICNp/8ODB8ng8uvzyyyVJo0eP1k033aSFCxeqvLxcPT09Kiws1Jw5c3iCBwAASLqAj3gOHjyo8ePHa/z48ZKkoqIijR8/XitWrPjax9i+fbtGjRqlqVOnatq0aZo8ebI2b97c11EAAMAA1ecrKFOmTJFlWV97/48++ui8dfHx8fxSNgAA8KX4Lh4AAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcWz9Lh70HyMe/IvTI1ywj9ZMd3oEAECIcQUFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGcTs9AIBvbsSDf3F6hAvy0ZrpTo8AwFBcQQEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcXjMGANaf338VuIRXGAg4N9BF44rKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADj9DlQ9u7dqxkzZsjr9crlcqmysjKwraenR8uWLdPYsWM1ZMgQeb1e/exnP9PJkyeDjnHq1Cnl5eUpJiZGcXFxWrBggTo7O7/xyQAAgIGhz4HS1dWl9PR0lZWVnbfts88+U2Njo5YvX67Gxkb9+c9/1pEjR/STn/wkaL+8vDy99957qq6u1q5du7R3714tWrTows8CAAAMKH3+Vfe5ubnKzc3tdVtsbKyqq6uD1j311FOaOHGijh8/rtTUVB0+fFhVVVU6cOCAMjIyJEkbN27UtGnTtH79enm93gs4DQAAMJCE/B6U9vZ2uVwuxcXFSZLq6uoUFxcXiBNJys7OVlhYmOrr63s9Rnd3tzo6OoIWAAAwcIU0UM6cOaNly5bpjjvuUExMjCTJ5/MpMTExaD+32634+Hj5fL5ej1NSUqLY2NjAkpKSEsqxAQCAw0IWKD09PbrttttkWZY2bdr0jY5VXFys9vb2wNLc3GzTlAAAwER9vgfl6/hvnHz88cfas2dP4OqJJHk8HrW2tgbt//nnn+vUqVPyeDy9Hi8iIkIRERGhGBUAABjI9iso/42To0eP6vXXX1dCQkLQ9qysLLW1tamhoSGwbs+ePfL7/crMzLR7HAAA0A/1+QpKZ2enjh07Fnjd1NSkQ4cOKT4+XsnJyfrpT3+qxsZG7dq1S+fOnQvcVxIfH6/w8HCNHj1aN910kxYuXKjy8nL19PSosLBQc+bM4QkeAAAg6QIC5eDBg7r++usDr4uKiiRJ+fn5evjhh/XKK69Ikq666qqg973xxhuaMmWKJGn79u0qLCzU1KlTFRYWptmzZ2vDhg0XeAoALhYjHvyL0yNckI/WTHd6BKDf6XOgTJkyRZZlfen2r9r2X/Hx8aqoqOjrjwYAABcJvosHAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABgnJL9JFgCAr9JfHxmXeGz828IVFAAAYBwCBQAAGIePeHrRXy89ctkRADBQcAUFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABinz4Gyd+9ezZgxQ16vVy6XS5WVlUHbLcvSihUrlJycrKioKGVnZ+vo0aNB+5w6dUp5eXmKiYlRXFycFixYoM7Ozm90IgAAYODoc6B0dXUpPT1dZWVlvW5ft26dNmzYoPLyctXX12vIkCHKycnRmTNnAvvk5eXpvffeU3V1tXbt2qW9e/dq0aJFF34WAABgQHH39Q25ubnKzc3tdZtlWSotLdVDDz2kmTNnSpKef/55JSUlqbKyUnPmzNHhw4dVVVWlAwcOKCMjQ5K0ceNGTZs2TevXr5fX6/0GpwMAAAYCW+9BaWpqks/nU3Z2dmBdbGysMjMzVVdXJ0mqq6tTXFxcIE4kKTs7W2FhYaqvr7dzHAAA0E/1+QrKV/H5fJKkpKSkoPVJSUmBbT6fT4mJicFDuN2Kj48P7PNF3d3d6u7uDrzu6Oiwc2wAAGCYfvEUT0lJiWJjYwNLSkqK0yMBAIAQsjVQPB6PJKmlpSVofUtLS2Cbx+NRa2tr0PbPP/9cp06dCuzzRcXFxWpvbw8szc3Ndo4NAAAMY2ugpKWlyePxqKamJrCuo6ND9fX1ysrKkiRlZWWpra1NDQ0NgX327Nkjv9+vzMzMXo8bERGhmJiYoAUAAAxcfb4HpbOzU8eOHQu8bmpq0qFDhxQfH6/U1FQtWbJEjzzyiEaOHKm0tDQtX75cXq9Xs2bNkiSNHj1aN910kxYuXKjy8nL19PSosLBQc+bM4QkeAAAg6QIC5eDBg7r++usDr4uKiiRJ+fn52rp1qx544AF1dXVp0aJFamtr0+TJk1VVVaXIyMjAe7Zv367CwkJNnTpVYWFhmj17tjZs2GDD6QAAgIGgz4EyZcoUWZb1pdtdLpdWrVqlVatWfek+8fHxqqio6OuPBgAAF4l+8RQPAAC4uBAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwju2Bcu7cOS1fvlxpaWmKiorSZZddpt/+9reyLCuwj2VZWrFihZKTkxUVFaXs7GwdPXrU7lEAAEA/ZXugrF27Vps2bdJTTz2lw4cPa+3atVq3bp02btwY2GfdunXasGGDysvLVV9fryFDhignJ0dnzpyxexwAANAPue0+4Ntvv62ZM2dq+vTpkqQRI0boj3/8o/bv3y/pP1dPSktL9dBDD2nmzJmSpOeff15JSUmqrKzUnDlz7B4JAAD0M7ZfQbnuuutUU1OjDz74QJL0j3/8Q2+99ZZyc3MlSU1NTfL5fMrOzg68JzY2VpmZmaqrq7N7HAAA0A/ZfgXlwQcfVEdHh0aNGqVBgwbp3LlzevTRR5WXlydJ8vl8kqSkpKSg9yUlJQW2fVF3d7e6u7sDrzs6OuweGwAAGMT2Kygvvviitm/froqKCjU2Nmrbtm1av369tm3bdsHHLCkpUWxsbGBJSUmxcWIAAGAa2wPl/vvv14MPPqg5c+Zo7Nixmjt3rpYuXaqSkhJJksfjkSS1tLQEva+lpSWw7YuKi4vV3t4eWJqbm+0eGwAAGMT2QPnss88UFhZ82EGDBsnv90uS0tLS5PF4VFNTE9je0dGh+vp6ZWVl9XrMiIgIxcTEBC0AAGDgsv0elBkzZujRRx9VamqqrrzySv3973/X448/rvnz50uSXC6XlixZokceeUQjR45UWlqali9fLq/Xq1mzZtk9DgAA6IdsD5SNGzdq+fLl+uUvf6nW1lZ5vV794he/0IoVKwL7PPDAA+rq6tKiRYvU1tamyZMnq6qqSpGRkXaPAwAA+iHbAyU6OlqlpaUqLS390n1cLpdWrVqlVatW2f3jAQDAAMB38QAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAME5IAuXEiRO68847lZCQoKioKI0dO1YHDx4MbLcsSytWrFBycrKioqKUnZ2to0ePhmIUAADQD9keKJ9++qkmTZqkwYMH67XXXtP777+vxx57TEOHDg3ss27dOm3YsEHl5eWqr6/XkCFDlJOTozNnztg9DgAA6Ifcdh9w7dq1SklJ0ZYtWwLr0tLSAn9tWZZKS0v10EMPaebMmZKk559/XklJSaqsrNScOXPsHgkAAPQztl9BeeWVV5SRkaFbb71ViYmJGj9+vJ599tnA9qamJvl8PmVnZwfWxcbGKjMzU3V1db0es7u7Wx0dHUELAAAYuGwPlA8//FCbNm3SyJEjtXv3bt1zzz267777tG3bNkmSz+eTJCUlJQW9LykpKbDti0pKShQbGxtYUlJS7B4bAAAYxPZA8fv9uvrqq7V69WqNHz9eixYt0sKFC1VeXn7BxywuLlZ7e3tgaW5utnFiAABgGtsDJTk5WVdccUXQutGjR+v48eOSJI/HI0lqaWkJ2qelpSWw7YsiIiIUExMTtAAAgIHL9kCZNGmSjhw5ErTugw8+0PDhwyX954ZZj8ejmpqawPaOjg7V19crKyvL7nEAAEA/ZPtTPEuXLtV1112n1atX67bbbtP+/fu1efNmbd68WZLkcrm0ZMkSPfLIIxo5cqTS0tK0fPlyeb1ezZo1y+5xAABAP2R7oFxzzTXauXOniouLtWrVKqWlpam0tFR5eXmBfR544AF1dXVp0aJFamtr0+TJk1VVVaXIyEi7xwEAAP2Q7YEiSTfffLNuvvnmL93ucrm0atUqrVq1KhQ/HgAA9HN8Fw8AADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOOEPFDWrFkjl8ulJUuWBNadOXNGBQUFSkhI0CWXXKLZs2erpaUl1KMAAIB+IqSBcuDAAT3zzDMaN25c0PqlS5fq1Vdf1UsvvaTa2lqdPHlSt9xySyhHAQAA/UjIAqWzs1N5eXl69tlnNXTo0MD69vZ2Pffcc3r88cd1ww03aMKECdqyZYvefvtt7du3L1TjAACAfiRkgVJQUKDp06crOzs7aH1DQ4N6enqC1o8aNUqpqamqq6vr9Vjd3d3q6OgIWgAAwMDlDsVBd+zYocbGRh04cOC8bT6fT+Hh4YqLiwtan5SUJJ/P1+vxSkpK9Jvf/CYUowIAAAPZfgWlublZixcv1vbt2xUZGWnLMYuLi9Xe3h5YmpubbTkuAAAwk+2B0tDQoNbWVl199dVyu91yu92qra3Vhg0b5Ha7lZSUpLNnz6qtrS3ofS0tLfJ4PL0eMyIiQjExMUELAAAYuGz/iGfq1Kl65513gtbNmzdPo0aN0rJly5SSkqLBgwerpqZGs2fPliQdOXJEx48fV1ZWlt3jAACAfsj2QImOjtaYMWOC1g0ZMkQJCQmB9QsWLFBRUZHi4+MVExOje++9V1lZWbr22mvtHgcAAPRDIblJ9n954oknFBYWptmzZ6u7u1s5OTl6+umnnRgFAAAY6FsJlDfffDPodWRkpMrKylRWVvZt/HgAANDP8F08AADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMY3uglJSU6JprrlF0dLQSExM1a9YsHTlyJGifM2fOqKCgQAkJCbrkkks0e/ZstbS02D0KAADop2wPlNraWhUUFGjfvn2qrq5WT0+PbrzxRnV1dQX2Wbp0qV599VW99NJLqq2t1cmTJ3XLLbfYPQoAAOin3HYfsKqqKuj11q1blZiYqIaGBv3f//2f2tvb9dxzz6miokI33HCDJGnLli0aPXq09u3bp2uvvdbukQAAQD8T8ntQ2tvbJUnx8fGSpIaGBvX09Cg7Ozuwz6hRo5Samqq6urpej9Hd3a2Ojo6gBQAADFwhDRS/368lS5Zo0qRJGjNmjCTJ5/MpPDxccXFxQfsmJSXJ5/P1epySkhLFxsYGlpSUlFCODQAAHBbSQCkoKNC7776rHTt2fKPjFBcXq729PbA0NzfbNCEAADCR7feg/FdhYaF27dqlvXv3atiwYYH1Ho9HZ8+eVVtbW9BVlJaWFnk8nl6PFRERoYiIiFCNCgAADGP7FRTLslRYWKidO3dqz549SktLC9o+YcIEDR48WDU1NYF1R44c0fHjx5WVlWX3OAAAoB+y/QpKQUGBKioq9PLLLys6OjpwX0lsbKyioqIUGxurBQsWqKioSPHx8YqJidG9996rrKwsnuABAACSQhAomzZtkiRNmTIlaP2WLVt01113SZKeeOIJhYWFafbs2eru7lZOTo6efvppu0cBAAD9lO2BYlnW/9wnMjJSZWVlKisrs/vHAwCAAYDv4gEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBxHA6WsrEwjRoxQZGSkMjMztX//fifHAQAAhnAsUF544QUVFRVp5cqVamxsVHp6unJyctTa2urUSAAAwBCOBcrjjz+uhQsXat68ebriiitUXl6u73znO/r973/v1EgAAMAQbid+6NmzZ9XQ0KDi4uLAurCwMGVnZ6uuru68/bu7u9Xd3R143d7eLknq6OgIyXz+7s9CctxQ68v/Hv31HCXOszf99Tz7+s8w52m2i+HPrMR52nFMy7L+986WA06cOGFJst5+++2g9ffff781ceLE8/ZfuXKlJYmFhYWFhYVlACzNzc3/sxUcuYLSV8XFxSoqKgq89vv9OnXqlBISEuRyuRycrG86OjqUkpKi5uZmxcTEOD1OyFwM53kxnKPEeQ40nOfA0V/P0bIsnT59Wl6v93/u60igXHrppRo0aJBaWlqC1re0tMjj8Zy3f0REhCIiIoLWxcXFhXLEkIqJielXf6Au1MVwnhfDOUqc50DDeQ4c/fEcY2Njv9Z+jtwkGx4ergkTJqimpiawzu/3q6amRllZWU6MBAAADOLYRzxFRUXKz89XRkaGJk6cqNLSUnV1dWnevHlOjQQAAAzhWKDcfvvt+te//qUVK1bI5/PpqquuUlVVlZKSkpwaKeQiIiK0cuXK8z6uGmguhvO8GM5R4jwHGs5z4LgYztFlWV/nWR8AAIBvD9/FAwAAjEOgAAAA4xAoAADAOAQKAAAwDoHyLSkrK9OIESMUGRmpzMxM7d+/3+mRbLd3717NmDFDXq9XLpdLlZWVTo9ku5KSEl1zzTWKjo5WYmKiZs2apSNHjjg9lu02bdqkcePGBX4JVFZWll577TWnxwqpNWvWyOVyacmSJU6PYquHH35YLpcraBk1apTTY4XEiRMndOeddyohIUFRUVEaO3asDh486PRYthoxYsR5fz9dLpcKCgqcHs12BMq34IUXXlBRUZFWrlypxsZGpaenKycnR62trU6PZquuri6lp6errKzM6VFCpra2VgUFBdq3b5+qq6vV09OjG2+8UV1dXU6PZqthw4ZpzZo1amho0MGDB3XDDTdo5syZeu+995weLSQOHDigZ555RuPGjXN6lJC48sor9cknnwSWt956y+mRbPfpp59q0qRJGjx4sF577TW9//77euyxxzR06FCnR7PVgQMHgv5eVldXS5JuvfVWhycLAXu+/g9fZeLEiVZBQUHg9blz5yyv12uVlJQ4OFVoSbJ27tzp9Bgh19raakmyamtrnR4l5IYOHWr97ne/c3oM250+fdoaOXKkVV1dbf34xz+2Fi9e7PRItlq5cqWVnp7u9Bght2zZMmvy5MlOj/GtW7x4sXXZZZdZfr/f6VFsxxWUEDt79qwaGhqUnZ0dWBcWFqbs7GzV1dU5OBns0N7eLkmKj493eJLQOXfunHbs2KGurq4B+VUUBQUFmj59etA/owPN0aNH5fV69f3vf195eXk6fvy40yPZ7pVXXlFGRoZuvfVWJSYmavz48Xr22WedHiukzp49qz/84Q+aP39+v/ri3K+LQAmxf//73zp37tx5vyE3KSlJPp/PoalgB7/fryVLlmjSpEkaM2aM0+PY7p133tEll1yiiIgI3X333dq5c6euuOIKp8ey1Y4dO9TY2KiSkhKnRwmZzMxMbd26VVVVVdq0aZOampr0ox/9SKdPn3Z6NFt9+OGH2rRpk0aOHKndu3frnnvu0X333adt27Y5PVrIVFZWqq2tTXfddZfTo4SEY7/qHujvCgoK9O677w7Iz/Ml6fLLL9ehQ4fU3t6uP/3pT8rPz1dtbe2AiZTm5mYtXrxY1dXVioyMdHqckMnNzQ389bhx45SZmanhw4frxRdf1IIFCxyczF5+v18ZGRlavXq1JGn8+PF69913VV5ervz8fIenC43nnntOubm58nq9To8SElxBCbFLL71UgwYNUktLS9D6lpYWeTweh6bCN1VYWKhdu3bpjTfe0LBhw5weJyTCw8P1gx/8QBMmTFBJSYnS09P15JNPOj2WbRoaGtTa2qqrr75abrdbbrdbtbW12rBhg9xut86dO+f0iCERFxenH/7whzp27JjTo9gqOTn5vHgePXr0gPw4S5I+/vhjvf766/r5z3/u9CghQ6CEWHh4uCZMmKCamprAOr/fr5qamgH5ef5AZ1mWCgsLtXPnTu3Zs0dpaWlOj/St8fv96u7udnoM20ydOlXvvPOODh06FFgyMjKUl5enQ4cOadCgQU6PGBKdnZ365z//qeTkZKdHsdWkSZPOe+T/gw8+0PDhwx2aKLS2bNmixMRETZ8+3elRQoaPeL4FRUVFys/PV0ZGhiZOnKjS0lJ1dXVp3rx5To9mq87OzqD/V9bU1KRDhw4pPj5eqampDk5mn4KCAlVUVOjll19WdHR04D6i2NhYRUVFOTydfYqLi5Wbm6vU1FSdPn1aFRUVevPNN7V7926nR7NNdHT0efcODRkyRAkJCQPqnqJf//rXmjFjhoYPH66TJ09q5cqVGjRokO644w6nR7PV0qVLdd1112n16tW67bbbtH//fm3evFmbN292ejTb+f1+bdmyRfn5+XK7B/B/xp1+jOhisXHjRis1NdUKDw+3Jk6caO3bt8/pkWz3xhtvWJLOW/Lz850ezTa9nZ8ka8uWLU6PZqv58+dbw4cPt8LDw63vfve71tSpU62//vWvTo8VcgPxMePbb7/dSk5OtsLDw63vfe971u23324dO3bM6bFC4tVXX7XGjBljRUREWKNGjbI2b97s9EghsXv3bkuSdeTIEadHCSmXZVmWM2kEAADQO+5BAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGOf/Aczmof5T4c97AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_counts = defaultdict(int)\n",
    "for i in range(1000):\n",
    "    sampled_idx = random.choices(train_labels, weights=sample_weights, k=1)[0]\n",
    "    sample_counts[sampled_idx] += 1\n",
    "\n",
    "# use matplotlib to plot the distribution\n",
    "\n",
    "plt.bar(sample_counts.keys(), sample_counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c704977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSamplerTrainer(Trainer):\n",
    "    def __init__(self, *args, train_sampler=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_sampler = train_sampler\n",
    "        \n",
    "    def _get_train_sampler(self, train_dataset: Dataset | None = None):\n",
    "        if train_dataset is None:\n",
    "            train_dataset = self.train_dataset\n",
    "            \n",
    "        if train_dataset is None or not has_length(train_dataset):\n",
    "            return None\n",
    "        \n",
    "        if self.train_sampler is not None:\n",
    "            return self.train_sampler\n",
    "        \n",
    "        return super()._get_train_sampler(train_dataset)\n",
    "\n",
    "trainer = BalancedSamplerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_sampler=sampler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5580d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnkosik11\u001b[0m (\u001b[33mnkosik11-hobby\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/wandb/run-20251205_002315-b6bdv69j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/b6bdv69j' target=\"_blank\">New Preprocessing - 10 Epochs - Balanced Sampler - Stratified Split</a></strong> to <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/b6bdv69j' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier/runs/b6bdv69j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7450' max='7450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7450/7450 14:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>F1 Lhhp</th>\n",
       "      <th>Precision Lhhp</th>\n",
       "      <th>Recall Lhhp</th>\n",
       "      <th>F1 Rhhp</th>\n",
       "      <th>Precision Rhhp</th>\n",
       "      <th>Recall Rhhp</th>\n",
       "      <th>F1 Lhmp</th>\n",
       "      <th>Precision Lhmp</th>\n",
       "      <th>Recall Lhmp</th>\n",
       "      <th>F1 Rhmp</th>\n",
       "      <th>Precision Rhmp</th>\n",
       "      <th>Recall Rhmp</th>\n",
       "      <th>F1 Lhblp</th>\n",
       "      <th>Precision Lhblp</th>\n",
       "      <th>Recall Lhblp</th>\n",
       "      <th>F1 Rhblp</th>\n",
       "      <th>Precision Rhblp</th>\n",
       "      <th>Recall Rhblp</th>\n",
       "      <th>F1 Lhbp</th>\n",
       "      <th>Precision Lhbp</th>\n",
       "      <th>Recall Lhbp</th>\n",
       "      <th>F1 Rhbp</th>\n",
       "      <th>Precision Rhbp</th>\n",
       "      <th>Recall Rhbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>2.265733</td>\n",
       "      <td>0.069799</td>\n",
       "      <td>0.065898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.111888</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.072607</td>\n",
       "      <td>0.039855</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.082262</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.033000</td>\n",
       "      <td>2.101100</td>\n",
       "      <td>0.068456</td>\n",
       "      <td>0.073978</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.045643</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.033200</td>\n",
       "      <td>2.222495</td>\n",
       "      <td>0.131544</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.247379</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.112821</td>\n",
       "      <td>0.065476</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.964000</td>\n",
       "      <td>2.044230</td>\n",
       "      <td>0.131544</td>\n",
       "      <td>0.145899</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.116402</td>\n",
       "      <td>0.063953</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.797600</td>\n",
       "      <td>1.998892</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>0.155565</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.057803</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.037313</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.167939</td>\n",
       "      <td>0.105769</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.485400</td>\n",
       "      <td>1.786829</td>\n",
       "      <td>0.222819</td>\n",
       "      <td>0.214087</td>\n",
       "      <td>0.048110</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.252033</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.197628</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.293706</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.224719</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.720300</td>\n",
       "      <td>0.303356</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.256198</td>\n",
       "      <td>0.287037</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.296774</td>\n",
       "      <td>0.248649</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>1.801469</td>\n",
       "      <td>0.284564</td>\n",
       "      <td>0.321908</td>\n",
       "      <td>0.153392</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>1.713678</td>\n",
       "      <td>0.362416</td>\n",
       "      <td>0.361122</td>\n",
       "      <td>0.323383</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.384384</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.273333</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>1.650539</td>\n",
       "      <td>0.445638</td>\n",
       "      <td>0.407964</td>\n",
       "      <td>0.530756</td>\n",
       "      <td>0.515358</td>\n",
       "      <td>0.547101</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.330097</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>1.750374</td>\n",
       "      <td>0.417450</td>\n",
       "      <td>0.424433</td>\n",
       "      <td>0.424628</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.288066</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.768000</td>\n",
       "      <td>0.489933</td>\n",
       "      <td>0.430088</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>1.852219</td>\n",
       "      <td>0.522148</td>\n",
       "      <td>0.480570</td>\n",
       "      <td>0.608964</td>\n",
       "      <td>0.530997</td>\n",
       "      <td>0.713768</td>\n",
       "      <td>0.443686</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.485075</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>1.970530</td>\n",
       "      <td>0.518121</td>\n",
       "      <td>0.467782</td>\n",
       "      <td>0.584459</td>\n",
       "      <td>0.547468</td>\n",
       "      <td>0.626812</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.450704</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7450, training_loss=1.1350535305714446, metrics={'train_runtime': 901.1522, 'train_samples_per_second': 66.126, 'train_steps_per_second': 8.267, 'total_flos': 7.425696207814263e+19, 'train_loss': 1.1350535305714446, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_NAME\"] = \"New Preprocessing - 10 Epochs - Balanced Sampler - Stratified Spli\"\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751fcb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 2.058974266052246, 'eval_accuracy': 0.48053691275167787, 'eval_macro_f1': 0.45521439683261034, 'eval_f1_LHHP': 0.5545927209705372, 'eval_precision_LHHP': 0.4878048780487805, 'eval_recall_LHHP': 0.642570281124498, 'eval_f1_RHHP': 0.47096774193548385, 'eval_precision_RHHP': 0.41714285714285715, 'eval_recall_RHHP': 0.5407407407407407, 'eval_f1_LHMP': 0.26881720430107525, 'eval_precision_LHMP': 0.3968253968253968, 'eval_recall_LHMP': 0.2032520325203252, 'eval_f1_RHMP': 0.46511627906976744, 'eval_precision_RHMP': 0.47058823529411764, 'eval_recall_RHMP': 0.45977011494252873, 'eval_f1_LHBlP': 0.5384615384615384, 'eval_precision_LHBlP': 0.6511627906976745, 'eval_recall_LHBlP': 0.45901639344262296, 'eval_f1_RHBlP': 0.68, 'eval_precision_RHBlP': 0.85, 'eval_recall_RHBlP': 0.5666666666666667, 'eval_f1_LHBP': 0.37209302325581395, 'eval_precision_LHBP': 0.42105263157894735, 'eval_recall_LHBP': 0.3333333333333333, 'eval_f1_RHBP': 0.2916666666666667, 'eval_precision_RHBP': 0.5833333333333334, 'eval_recall_RHBP': 0.19444444444444445, 'eval_runtime': 8.1703, 'eval_samples_per_second': 91.184, 'eval_steps_per_second': 11.505, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test split\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c80814",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
