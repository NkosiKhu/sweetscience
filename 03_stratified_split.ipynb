{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a10ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import av  # pip install av\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import evaluate  # pip install evaluate\n",
    "\n",
    "# load environment variables with dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5398097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5df0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Point this at the Olympic Boxing dataset directory\n",
    "\n",
    "# Pretrained VideoMAE base (self-supervised on K400)\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL2ID),\n",
    "    label2id=LABEL2ID,\n",
    "    id2label=ID2LABEL,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79ab45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14677eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def split_data():\n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "    test_paths = []\n",
    "\n",
    "    # Collect all paths with their labels and sources\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    all_sources = []\n",
    "\n",
    "    for label in os.listdir(\"preprocessed_clips_3\"):\n",
    "        paths = [f\"preprocessed_clips_3/{label}/{p}\" for p in os.listdir(f\"preprocessed_clips_3/{label}\")]\n",
    "        \n",
    "        for path in paths:\n",
    "            # Extract source from filename: pattern is clip_task_[kamx_nums]_index_c.npy\n",
    "            # Source is the part matching task_[kamx_nums]_index_c (everything from task_ to .npy)\n",
    "            filename = os.path.basename(path)\n",
    "            # Match task_ followed by any characters until .npy\n",
    "            source_match = re.search(r'task_kam\\d+_[^_]+', filename)\n",
    "            if source_match:\n",
    "                source = source_match.group(0)\n",
    "            else:\n",
    "                # Fallback: use filename without extension as source\n",
    "                source = os.path.splitext(filename)[0]\n",
    "            \n",
    "            all_paths.append(path)\n",
    "            all_labels.append(label)\n",
    "            all_sources.append(source)\n",
    "\n",
    "    # Create combined stratification key: label_source\n",
    "    # This ensures both label and source distributions are maintained\n",
    "    stratify_key = [f\"{label}_{source}\" for label, source in zip(all_labels, all_sources)]\n",
    "\n",
    "\n",
    "    # First split: 80% train, 20% temp (which will become val+test)\n",
    "    train_paths, temp_paths, train_labels, temp_labels, train_sources, temp_sources = train_test_split(\n",
    "        all_paths, all_labels, all_sources,\n",
    "        test_size=0.2,\n",
    "        stratify=stratify_key,\n",
    "        random_state=632\n",
    "    )\n",
    "\n",
    "    # Second split: split temp into 50% val, 50% test (which gives 10% val, 10% test overall)\n",
    "    # Create new stratification key for temp split\n",
    "    temp_stratify_key = [f\"{label}_{source}\" for label, source in zip(temp_labels, temp_sources)]\n",
    "\n",
    "    temp_counts = Counter(temp_stratify_key)\n",
    "    min_count = min(temp_counts.values())\n",
    "\n",
    "    if min_count >= 2:\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=temp_stratify_key,\n",
    "            random_state=632\n",
    "        )\n",
    "    else:\n",
    "        # Fall back to non-stratified split if some classes have < 2 members\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=None,\n",
    "            random_state=632\n",
    "        )\n",
    "\n",
    "    # Convert to class variables\n",
    "    train_paths = train_paths\n",
    "    val_paths = val_paths\n",
    "    test_paths = test_paths\n",
    "    \n",
    "    return train_paths, val_paths, test_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ede674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import random\n",
    "class BoxingDataset(Dataset):\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    all_splits = split_data()\n",
    "    train_paths = all_splits[0]\n",
    "    val_paths = all_splits[1]\n",
    "    test_paths = all_splits[2]\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self, split: str):\n",
    "        self.split = split\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.train_paths)\n",
    "        elif self.split == \"val\":\n",
    "            return len(self.val_paths)\n",
    "        elif self.split == \"test\":\n",
    "            return len(self.test_paths)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            path = self.train_paths[idx]\n",
    "        elif self.split == \"val\":\n",
    "            path = self.val_paths[idx]\n",
    "        elif self.split == \"test\":\n",
    "            path = self.test_paths[idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "        \n",
    "        clip = np.load(path)\n",
    "        \n",
    "        # convert to float and scale to 0-1\n",
    "        clip = clip.astype(np.float32) / 255.0\n",
    "        \n",
    "        # image net mean/std\n",
    "        clip = (clip - self.mean) / self.std\n",
    "        \n",
    "        #reorder to (T,C,H,W)\n",
    "        clip = clip.transpose(0,3,1,2)\n",
    "        \n",
    "        #convert to tensor\n",
    "        clip = torch.from_numpy(clip)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": clip,\n",
    "            \"labels\": torch.tensor(LABEL2ID[path.split(\"/\")[-2]], dtype=torch.long) \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b171cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BoxingDataset(\n",
    "    split=\"train\",\n",
    ")\n",
    "val_dataset = BoxingDataset(\n",
    "    split=\"val\",\n",
    ")\n",
    "test_dataset = BoxingDataset(\n",
    "    split=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bd3255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19640"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a143d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FACTS used batch_size=4, grad_accum=2, warmup_ratio=0.1, epochs=10\n",
    "# Learning rate is not rendered in the HTML; start with 1e-4 and tune around it.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./facts-boxing-videomae\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=24, \n",
    "    per_device_eval_batch_size=24, \n",
    "    gradient_accumulation_steps=1,  \n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"wandb\",  # or \"wandb\"/\"tensorboard\"\n",
    "    dataloader_num_workers=4,        # ADD THIS - use multiple workers\n",
    "    dataloader_pin_memory=True,      # ADD THIS - faster CPU->GPU transfer\n",
    "    dataloader_prefetch_factor=2, \n",
    "    torch_compile=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "data_collator = VideoDataCollator()\n",
    "\n",
    "train_labels = [LABEL2ID[path.split(\"/\")[-2]] for path in BoxingDataset.train_paths]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(LABEL2ID)),\n",
    "    y=np.array(train_labels)  # Ensure it's a numpy array\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a544dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x7043c47df9a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    sample_weights,                                              \n",
    "    len(sample_weights), \n",
    "    replacement=True\n",
    ")\n",
    "sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0d03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# sample_counts = defaultdict(int)\n",
    "# for i in range(500):\n",
    "#     sampled_idx = random.choices(train_labels, weights=sample_weights, k=1)[0]\n",
    "#     sample_counts[sampled_idx] += 1\n",
    "\n",
    "# # use matplotlib to plot the distribution\n",
    "\n",
    "# plt.bar(sample_counts.keys(), sample_counts.values())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c704977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSamplerTrainer(Trainer):\n",
    "    def __init__(self, *args, train_sampler=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_sampler = train_sampler\n",
    "        \n",
    "    def _get_train_sampler(self, train_dataset: Dataset | None = None):\n",
    "        if train_dataset is None:\n",
    "            train_dataset = self.train_dataset\n",
    "            \n",
    "        if train_dataset is None or not has_length(train_dataset):\n",
    "            return None\n",
    "        \n",
    "        if self.train_sampler is not None:\n",
    "            return self.train_sampler\n",
    "        \n",
    "        return super()._get_train_sampler(train_dataset)\n",
    "\n",
    "trainer = BalancedSamplerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_sampler=sampler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5580d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnkosik11\u001b[0m (\u001b[33mnkosik11-hobby\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/wandb/run-20251211_132128-erpnb9t9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/erpnb9t9' target=\"_blank\">20% Jitter Preprocessing - 15 Epochs - 5e-5 - Balanced Sampler - Stratified Split</a></strong> to <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/erpnb9t9' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier/runs/erpnb9t9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12285' max='12285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12285/12285 58:23, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>F1 Lhhp</th>\n",
       "      <th>Precision Lhhp</th>\n",
       "      <th>Recall Lhhp</th>\n",
       "      <th>F1 Rhhp</th>\n",
       "      <th>Precision Rhhp</th>\n",
       "      <th>Recall Rhhp</th>\n",
       "      <th>F1 Lhmp</th>\n",
       "      <th>Precision Lhmp</th>\n",
       "      <th>Recall Lhmp</th>\n",
       "      <th>F1 Rhmp</th>\n",
       "      <th>Precision Rhmp</th>\n",
       "      <th>Recall Rhmp</th>\n",
       "      <th>F1 Lhblp</th>\n",
       "      <th>Precision Lhblp</th>\n",
       "      <th>Recall Lhblp</th>\n",
       "      <th>F1 Rhblp</th>\n",
       "      <th>Precision Rhblp</th>\n",
       "      <th>Recall Rhblp</th>\n",
       "      <th>F1 Lhbp</th>\n",
       "      <th>Precision Lhbp</th>\n",
       "      <th>Recall Lhbp</th>\n",
       "      <th>F1 Rhbp</th>\n",
       "      <th>Precision Rhbp</th>\n",
       "      <th>Recall Rhbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.076000</td>\n",
       "      <td>2.083955</td>\n",
       "      <td>0.128310</td>\n",
       "      <td>0.112180</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>0.334821</td>\n",
       "      <td>0.087108</td>\n",
       "      <td>0.109929</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>0.073113</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>0.203488</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.369811</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.168269</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>0.051491</td>\n",
       "      <td>0.231707</td>\n",
       "      <td>0.092391</td>\n",
       "      <td>0.062271</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.058140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.053200</td>\n",
       "      <td>2.086711</td>\n",
       "      <td>0.129124</td>\n",
       "      <td>0.105999</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.256767</td>\n",
       "      <td>0.191024</td>\n",
       "      <td>0.391509</td>\n",
       "      <td>0.160982</td>\n",
       "      <td>0.197324</td>\n",
       "      <td>0.135945</td>\n",
       "      <td>0.135283</td>\n",
       "      <td>0.131206</td>\n",
       "      <td>0.139623</td>\n",
       "      <td>0.087977</td>\n",
       "      <td>0.112782</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.054201</td>\n",
       "      <td>0.035336</td>\n",
       "      <td>0.116279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.025300</td>\n",
       "      <td>2.102880</td>\n",
       "      <td>0.114868</td>\n",
       "      <td>0.106715</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.110899</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.068396</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.233010</td>\n",
       "      <td>0.110599</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.195462</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.110429</td>\n",
       "      <td>0.063604</td>\n",
       "      <td>0.418605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.604500</td>\n",
       "      <td>1.724771</td>\n",
       "      <td>0.269654</td>\n",
       "      <td>0.248461</td>\n",
       "      <td>0.371681</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.194079</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.139151</td>\n",
       "      <td>0.132110</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>0.290258</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>0.275472</td>\n",
       "      <td>0.296810</td>\n",
       "      <td>0.208577</td>\n",
       "      <td>0.514423</td>\n",
       "      <td>0.215743</td>\n",
       "      <td>0.141762</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.184486</td>\n",
       "      <td>0.115183</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>0.199262</td>\n",
       "      <td>0.627907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.050200</td>\n",
       "      <td>1.500687</td>\n",
       "      <td>0.356415</td>\n",
       "      <td>0.392248</td>\n",
       "      <td>0.272401</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.176539</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>0.387173</td>\n",
       "      <td>0.384434</td>\n",
       "      <td>0.354669</td>\n",
       "      <td>0.325626</td>\n",
       "      <td>0.389401</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>0.354651</td>\n",
       "      <td>0.460377</td>\n",
       "      <td>0.377104</td>\n",
       "      <td>0.290155</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.752900</td>\n",
       "      <td>1.388870</td>\n",
       "      <td>0.426477</td>\n",
       "      <td>0.460230</td>\n",
       "      <td>0.361545</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.255517</td>\n",
       "      <td>0.429967</td>\n",
       "      <td>0.398390</td>\n",
       "      <td>0.466981</td>\n",
       "      <td>0.412183</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>0.443662</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.475472</td>\n",
       "      <td>0.470309</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.475962</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.486726</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.287823</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.573840</td>\n",
       "      <td>0.450331</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>1.306968</td>\n",
       "      <td>0.461507</td>\n",
       "      <td>0.500738</td>\n",
       "      <td>0.446441</td>\n",
       "      <td>0.551195</td>\n",
       "      <td>0.375145</td>\n",
       "      <td>0.416999</td>\n",
       "      <td>0.477204</td>\n",
       "      <td>0.370283</td>\n",
       "      <td>0.400468</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.394009</td>\n",
       "      <td>0.524540</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.645283</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.345178</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>1.245615</td>\n",
       "      <td>0.506314</td>\n",
       "      <td>0.542429</td>\n",
       "      <td>0.490358</td>\n",
       "      <td>0.602369</td>\n",
       "      <td>0.413473</td>\n",
       "      <td>0.479508</td>\n",
       "      <td>0.423913</td>\n",
       "      <td>0.551887</td>\n",
       "      <td>0.465957</td>\n",
       "      <td>0.432806</td>\n",
       "      <td>0.504608</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.480938</td>\n",
       "      <td>0.618868</td>\n",
       "      <td>0.534759</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.557078</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>1.180005</td>\n",
       "      <td>0.542159</td>\n",
       "      <td>0.572853</td>\n",
       "      <td>0.540973</td>\n",
       "      <td>0.634375</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.530777</td>\n",
       "      <td>0.463845</td>\n",
       "      <td>0.620283</td>\n",
       "      <td>0.479657</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.552876</td>\n",
       "      <td>0.543796</td>\n",
       "      <td>0.562264</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.627737</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.698225</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.686047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>1.180400</td>\n",
       "      <td>0.553564</td>\n",
       "      <td>0.577278</td>\n",
       "      <td>0.549700</td>\n",
       "      <td>0.645768</td>\n",
       "      <td>0.478513</td>\n",
       "      <td>0.550598</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.596698</td>\n",
       "      <td>0.499494</td>\n",
       "      <td>0.445045</td>\n",
       "      <td>0.569124</td>\n",
       "      <td>0.567669</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.569811</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.540441</td>\n",
       "      <td>0.706731</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.557895</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>1.158740</td>\n",
       "      <td>0.564562</td>\n",
       "      <td>0.584561</td>\n",
       "      <td>0.569075</td>\n",
       "      <td>0.626220</td>\n",
       "      <td>0.521487</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530702</td>\n",
       "      <td>0.570755</td>\n",
       "      <td>0.505176</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.562212</td>\n",
       "      <td>0.593250</td>\n",
       "      <td>0.560403</td>\n",
       "      <td>0.630189</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>0.576763</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>1.130778</td>\n",
       "      <td>0.569450</td>\n",
       "      <td>0.587485</td>\n",
       "      <td>0.591187</td>\n",
       "      <td>0.624838</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.539554</td>\n",
       "      <td>0.473310</td>\n",
       "      <td>0.627358</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.497835</td>\n",
       "      <td>0.529954</td>\n",
       "      <td>0.560484</td>\n",
       "      <td>0.601732</td>\n",
       "      <td>0.524528</td>\n",
       "      <td>0.628297</td>\n",
       "      <td>0.626794</td>\n",
       "      <td>0.629808</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>1.155516</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.592814</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.646880</td>\n",
       "      <td>0.493612</td>\n",
       "      <td>0.553419</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.610849</td>\n",
       "      <td>0.531027</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.621160</td>\n",
       "      <td>0.566978</td>\n",
       "      <td>0.686792</td>\n",
       "      <td>0.621027</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>0.610577</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.514620</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>1.130274</td>\n",
       "      <td>0.586558</td>\n",
       "      <td>0.596286</td>\n",
       "      <td>0.609860</td>\n",
       "      <td>0.640665</td>\n",
       "      <td>0.581882</td>\n",
       "      <td>0.567714</td>\n",
       "      <td>0.525050</td>\n",
       "      <td>0.617925</td>\n",
       "      <td>0.512764</td>\n",
       "      <td>0.494647</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.611212</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.637736</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.519337</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>1.137428</td>\n",
       "      <td>0.575153</td>\n",
       "      <td>0.587569</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.636735</td>\n",
       "      <td>0.543554</td>\n",
       "      <td>0.558671</td>\n",
       "      <td>0.499072</td>\n",
       "      <td>0.634434</td>\n",
       "      <td>0.536485</td>\n",
       "      <td>0.484230</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.571984</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.554717</td>\n",
       "      <td>0.639386</td>\n",
       "      <td>0.683060</td>\n",
       "      <td>0.600962</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.478528</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.410526</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>1.123965</td>\n",
       "      <td>0.594705</td>\n",
       "      <td>0.603715</td>\n",
       "      <td>0.621096</td>\n",
       "      <td>0.630383</td>\n",
       "      <td>0.612079</td>\n",
       "      <td>0.570838</td>\n",
       "      <td>0.508287</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>0.534262</td>\n",
       "      <td>0.538642</td>\n",
       "      <td>0.529954</td>\n",
       "      <td>0.587084</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.654822</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.620192</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.639535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>1.137897</td>\n",
       "      <td>0.580041</td>\n",
       "      <td>0.586689</td>\n",
       "      <td>0.596598</td>\n",
       "      <td>0.625478</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.571121</td>\n",
       "      <td>0.525794</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.535005</td>\n",
       "      <td>0.489484</td>\n",
       "      <td>0.589862</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.581132</td>\n",
       "      <td>0.618926</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.581731</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.389474</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>1.143914</td>\n",
       "      <td>0.588595</td>\n",
       "      <td>0.601511</td>\n",
       "      <td>0.594969</td>\n",
       "      <td>0.648834</td>\n",
       "      <td>0.549361</td>\n",
       "      <td>0.571730</td>\n",
       "      <td>0.517176</td>\n",
       "      <td>0.639151</td>\n",
       "      <td>0.555440</td>\n",
       "      <td>0.504708</td>\n",
       "      <td>0.617512</td>\n",
       "      <td>0.600746</td>\n",
       "      <td>0.594096</td>\n",
       "      <td>0.607547</td>\n",
       "      <td>0.637975</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.479042</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>1.134276</td>\n",
       "      <td>0.592668</td>\n",
       "      <td>0.600747</td>\n",
       "      <td>0.619247</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.560825</td>\n",
       "      <td>0.498168</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.548753</td>\n",
       "      <td>0.540179</td>\n",
       "      <td>0.557604</td>\n",
       "      <td>0.579767</td>\n",
       "      <td>0.598394</td>\n",
       "      <td>0.562264</td>\n",
       "      <td>0.648101</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.502994</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>1.129921</td>\n",
       "      <td>0.593890</td>\n",
       "      <td>0.598946</td>\n",
       "      <td>0.628303</td>\n",
       "      <td>0.635392</td>\n",
       "      <td>0.621370</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.503676</td>\n",
       "      <td>0.646226</td>\n",
       "      <td>0.539977</td>\n",
       "      <td>0.543124</td>\n",
       "      <td>0.536866</td>\n",
       "      <td>0.571992</td>\n",
       "      <td>0.599174</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.642674</td>\n",
       "      <td>0.690608</td>\n",
       "      <td>0.600962</td>\n",
       "      <td>0.688742</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>1.131688</td>\n",
       "      <td>0.590224</td>\n",
       "      <td>0.597275</td>\n",
       "      <td>0.618031</td>\n",
       "      <td>0.631515</td>\n",
       "      <td>0.605110</td>\n",
       "      <td>0.572641</td>\n",
       "      <td>0.520231</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.529876</td>\n",
       "      <td>0.518764</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.573099</td>\n",
       "      <td>0.592742</td>\n",
       "      <td>0.554717</td>\n",
       "      <td>0.650124</td>\n",
       "      <td>0.671795</td>\n",
       "      <td>0.629808</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>1.135512</td>\n",
       "      <td>0.589002</td>\n",
       "      <td>0.596015</td>\n",
       "      <td>0.609639</td>\n",
       "      <td>0.633292</td>\n",
       "      <td>0.587689</td>\n",
       "      <td>0.571130</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.643868</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>0.520170</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.591603</td>\n",
       "      <td>0.598456</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.569767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>1.134780</td>\n",
       "      <td>0.591039</td>\n",
       "      <td>0.600942</td>\n",
       "      <td>0.612048</td>\n",
       "      <td>0.635795</td>\n",
       "      <td>0.590012</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.518234</td>\n",
       "      <td>0.636792</td>\n",
       "      <td>0.539648</td>\n",
       "      <td>0.516878</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.588910</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.581132</td>\n",
       "      <td>0.643216</td>\n",
       "      <td>0.673684</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.679739</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>1.132916</td>\n",
       "      <td>0.593483</td>\n",
       "      <td>0.601285</td>\n",
       "      <td>0.617034</td>\n",
       "      <td>0.633252</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.575026</td>\n",
       "      <td>0.517958</td>\n",
       "      <td>0.646226</td>\n",
       "      <td>0.539503</td>\n",
       "      <td>0.528761</td>\n",
       "      <td>0.550691</td>\n",
       "      <td>0.588008</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.573585</td>\n",
       "      <td>0.644670</td>\n",
       "      <td>0.682796</td>\n",
       "      <td>0.610577</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.616279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12285, training_loss=0.6296214087784752, metrics={'train_runtime': 3514.7393, 'train_samples_per_second': 83.818, 'train_steps_per_second': 3.495, 'total_flos': 3.6711027065314345e+20, 'train_loss': 0.6296214087784752, 'epoch': 15.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_NAME\"] = \"20% Jitter Preprocessing - 15 Epochs - 5e-5 - Balanced Sampler - Stratified Split\"\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751fcb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 1.081895351409912, 'eval_accuracy': 0.617671009771987, 'eval_macro_f1': 0.6118197703066269, 'eval_f1_LHHP': 0.653556969346443, 'eval_precision_LHHP': 0.6758373205741627, 'eval_recall_LHHP': 0.6326987681970885, 'eval_f1_RHHP': 0.6266009852216748, 'eval_precision_RHHP': 0.5608465608465608, 'eval_recall_RHHP': 0.7098214285714286, 'eval_f1_LHMP': 0.5467800729040098, 'eval_precision_LHMP': 0.5447941888619855, 'eval_recall_LHMP': 0.5487804878048781, 'eval_f1_RHMP': 0.5935483870967742, 'eval_precision_RHMP': 0.5948275862068966, 'eval_recall_RHMP': 0.592274678111588, 'eval_f1_LHBlP': 0.6131805157593123, 'eval_precision_LHBlP': 0.6331360946745562, 'eval_recall_LHBlP': 0.5944444444444444, 'eval_f1_RHBlP': 0.686046511627907, 'eval_precision_RHBlP': 0.8082191780821918, 'eval_recall_RHBlP': 0.5959595959595959, 'eval_f1_LHBP': 0.4891304347826087, 'eval_precision_LHBP': 0.5056179775280899, 'eval_recall_LHBP': 0.47368421052631576, 'eval_f1_RHBP': 0.6857142857142857, 'eval_precision_RHBP': 0.7792207792207793, 'eval_recall_RHBP': 0.6122448979591837, 'eval_runtime': 20.3309, 'eval_samples_per_second': 120.801, 'eval_steps_per_second': 5.066, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test split\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c80814",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
