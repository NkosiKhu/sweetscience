{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b2a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import av\n",
    "import os\n",
    "\n",
    "def create_grid_video_from_paths(paths: list, output_path: str, \n",
    "                                 fps: int = 8, margin: int = 10):\n",
    "    \"\"\"\n",
    "    Create a 5x5 grid video from a list of clip paths.\n",
    "    \n",
    "    Args:\n",
    "        paths: list of paths to .npy clip files\n",
    "        output_path: path to save the output video\n",
    "        fps: frames per second for output video\n",
    "        margin: pixel margin between grid cells\n",
    "    \"\"\"\n",
    "    # Select 25 videos equally distributed across the provided paths\n",
    "    num_videos = 25  # 5x5 grid\n",
    "    num_total = len(paths)\n",
    "    if num_total < num_videos:\n",
    "        print(f\"Warning: Only {num_total} videos available, using all of them\")\n",
    "        selected_paths = paths\n",
    "        grid_size = int(np.ceil(np.sqrt(num_total)))\n",
    "    else:\n",
    "        indices = np.linspace(0, num_total - 1, num_videos, dtype=int)\n",
    "        selected_paths = [paths[i] for i in indices]\n",
    "        grid_size = 5\n",
    "    \n",
    "    # Load all clips\n",
    "    clips = []\n",
    "    for path in selected_paths:\n",
    "        clip = np.load(path)  # Shape: (16, 224, 224, 3)\n",
    "        # Ensure uint8 format\n",
    "        if clip.dtype != np.uint8:\n",
    "            clip = np.clip(clip, 0, 255).astype(np.uint8)\n",
    "        clips.append(clip)\n",
    "    \n",
    "    # Get dimensions\n",
    "    T, H, W, C = clips[0].shape  # T=16, H=224, W=224, C=3\n",
    "    cell_h, cell_w = H, W  # 224x224\n",
    "    \n",
    "    # Calculate output dimensions with margins\n",
    "    output_h = grid_size * cell_h + (grid_size - 1) * margin\n",
    "    output_w = grid_size * cell_w + (grid_size - 1) * margin\n",
    "    \n",
    "    # Create output container\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    container = av.open(output_path, mode='w')\n",
    "    stream = container.add_stream('libx264', rate=fps)\n",
    "    stream.width = output_w\n",
    "    stream.height = output_h\n",
    "    stream.pix_fmt = 'yuv420p'\n",
    "    \n",
    "    # Create grid for each frame\n",
    "    for frame_idx in range(T):\n",
    "        # Create empty canvas for this frame (white background)\n",
    "        grid_frame = np.ones((output_h, output_w, C), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Place each clip's frame in the grid\n",
    "        for grid_idx, clip in enumerate(clips):\n",
    "            row = grid_idx // grid_size\n",
    "            col = grid_idx % grid_size\n",
    "            \n",
    "            # Position in output frame (accounting for margins)\n",
    "            y_start = row * (cell_h + margin)\n",
    "            y_end = y_start + cell_h\n",
    "            x_start = col * (cell_w + margin)\n",
    "            x_end = x_start + cell_w\n",
    "            \n",
    "            # Place the frame\n",
    "            grid_frame[y_start:y_end, x_start:x_end] = clip[frame_idx]\n",
    "        \n",
    "        # Convert to av.VideoFrame and encode\n",
    "        frame = av.VideoFrame.from_ndarray(grid_frame, format='rgb24')\n",
    "        for packet in stream.encode(frame):\n",
    "            container.mux(packet)\n",
    "    \n",
    "    # Flush encoder\n",
    "    for packet in stream.encode():\n",
    "        container.mux(packet)\n",
    "    \n",
    "    container.close()\n",
    "    print(f\"Grid video saved to {output_path} ({output_w}x{output_h}, {T} frames, {len(clips)} clips)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LHHP': 7005, 'RHMP': 2004, 'RHBP': 727, 'RHHP': 3486, 'LHBlP': 1546, 'LHMP': 3373, 'RHBlP': 738, 'LHBP': 761}\n"
     ]
    }
   ],
   "source": [
    "from train import *\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_data():\n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "    test_paths = []\n",
    "\n",
    "    # Collect all paths with their labels and sources\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    all_sources = []\n",
    "\n",
    "    for label in os.listdir(\"preprocessed_clips_3\"):\n",
    "        paths = [f\"preprocessed_clips_3/{label}/{p}\" for p in os.listdir(f\"preprocessed_clips_3/{label}\")]\n",
    "        \n",
    "        for path in paths:\n",
    "            # Extract source from filename: pattern is clip_task_[kamx_nums]_index_c.npy\n",
    "            # Source is the part matching task_[kamx_nums]_index_c (everything from task_ to .npy)\n",
    "            filename = os.path.basename(path)\n",
    "            # Match task_ followed by any characters until .npy\n",
    "            source_match = re.search(r'task_kam\\d+_[^_]+', filename)\n",
    "            if source_match:\n",
    "                source = source_match.group(0)\n",
    "            else:\n",
    "                # Fallback: use filename without extension as source\n",
    "                source = os.path.splitext(filename)[0]\n",
    "            \n",
    "            all_paths.append(path)\n",
    "            all_labels.append(label)\n",
    "            all_sources.append(source)\n",
    "\n",
    "    # Create combined stratification key: label_source\n",
    "    # This ensures both label and source distributions are maintained\n",
    "    stratify_key = [f\"{label}_{source}\" for label, source in zip(all_labels, all_sources)]\n",
    "\n",
    "\n",
    "    # First split: 80% train, 20% temp (which will become val+test)\n",
    "    train_paths, temp_paths, train_labels, temp_labels, train_sources, temp_sources = train_test_split(\n",
    "        all_paths, all_labels, all_sources,\n",
    "        test_size=0.2,\n",
    "        stratify=stratify_key,\n",
    "        random_state=632\n",
    "    )\n",
    "\n",
    "    # Second split: split temp into 50% val, 50% test (which gives 10% val, 10% test overall)\n",
    "    # Create new stratification key for temp split\n",
    "    temp_stratify_key = [f\"{label}_{source}\" for label, source in zip(temp_labels, temp_sources)]\n",
    "\n",
    "    temp_counts = Counter(temp_stratify_key)\n",
    "    min_count = min(temp_counts.values())\n",
    "\n",
    "    if min_count >= 2:\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=temp_stratify_key,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # Fall back to non-stratified split if some classes have < 2 members\n",
    "        val_paths, test_paths, val_labels, test_labels, val_sources, test_sources = train_test_split(\n",
    "            temp_paths, temp_labels, temp_sources,\n",
    "            test_size=0.5,\n",
    "            stratify=None,\n",
    "            random_state=632\n",
    "        )\n",
    "\n",
    "    # Convert to class variables\n",
    "    train_paths = train_paths\n",
    "    val_paths = val_paths\n",
    "    test_paths = test_paths\n",
    "    \n",
    "    return train_paths, val_paths, test_paths\n",
    "\n",
    "class BoxingDataset(Dataset):\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    all_splits = split_data()\n",
    "    train_paths = all_splits[0]\n",
    "    val_paths = all_splits[1]\n",
    "    test_paths = all_splits[2]\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self, split: str):\n",
    "        self.split = split\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.train_paths)\n",
    "        elif self.split == \"val\":\n",
    "            return len(self.val_paths)\n",
    "        elif self.split == \"test\":\n",
    "            return len(self.test_paths)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            path = self.train_paths[idx]\n",
    "        elif self.split == \"val\":\n",
    "            path = self.val_paths[idx]\n",
    "        elif self.split == \"test\":\n",
    "            path = self.test_paths[idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "        \n",
    "        clip = np.load(path)\n",
    "        \n",
    "        # convert to float and scale to 0-1\n",
    "        clip = clip.astype(np.float32) / 255.0\n",
    "        \n",
    "        # image net mean/std\n",
    "        clip = (clip - self.mean) / self.std\n",
    "        \n",
    "        #reorder to (T,C,H,W)\n",
    "        clip = clip.transpose(0,3,1,2)\n",
    "        \n",
    "        #convert to tensor\n",
    "        clip = torch.from_numpy(clip)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": clip,\n",
    "            \"labels\": torch.tensor(LABEL2ID[path.split(\"/\")[-2]], dtype=torch.long) \n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "paths = BoxingDataset.train_paths\n",
    "label_counts = defaultdict(int)\n",
    "for path in paths:\n",
    "    label = path.split(\"/\")[-2]\n",
    "    label_counts[label] += 1\n",
    "\n",
    "print(dict(label_counts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debd0451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid video saved to grid_vids_4/RHMP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/RHBP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/LHBlP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/RHBlP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/LHBP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/LHMP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/RHHP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/LHHP_grid.mp4 (1160x1160, 16 frames, 25 clips)\n"
     ]
    }
   ],
   "source": [
    "for label in os.listdir(\"preprocessed_clips_3\"):\n",
    "    paths = [f\"preprocessed_clips_3/{label}/{p}\" for p in os.listdir(f\"preprocessed_clips_3/{label}\")]\n",
    "    create_grid_video_from_paths(paths, f'grid_vids_4/{label}_grid.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee5449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid video saved to grid_vids_4/train_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/val_grid.mp4 (1160x1160, 16 frames, 25 clips)\n",
      "Grid video saved to grid_vids_4/test_grid.mp4 (1160x1160, 16 frames, 25 clips)\n"
     ]
    }
   ],
   "source": [
    "train_paths = BoxingDataset.train_paths\n",
    "val_paths = BoxingDataset.val_paths\n",
    "test_paths = BoxingDataset.test_paths\n",
    "\n",
    "for split, paths_list in [('train',train_paths), ('val',val_paths), ('test',test_paths)]:\n",
    "    paths = paths_list\n",
    "    create_grid_video_from_paths(paths, f'grid_vids_4/{split}_grid.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f046e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
