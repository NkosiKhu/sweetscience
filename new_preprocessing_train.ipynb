{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a10ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import av  # pip install av\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import evaluate  # pip install evaluate\n",
    "\n",
    "# load environment variables with dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5398097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5df0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Point this at the Olympic Boxing dataset directory\n",
    "\n",
    "# Pretrained VideoMAE base (self-supervised on K400)\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL2ID),\n",
    "    label2id=LABEL2ID,\n",
    "    id2label=ID2LABEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79ab45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14677eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocessed_clips_2/LHHP/clip_task_kam2_gh118416_10110_0.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in os.listdir(\"preprocessed_clips_2\"):\n",
    "    paths = (lambda x: [f\"preprocessed_clips_2/{x}/{p}\" for p in os.listdir(f\"preprocessed_clips_2/{x}\")])(label)\n",
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ede674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocessed_clips_2/RHBP/clip_task_kam2_gh178416_6184_0.npy'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoxingDataset(Dataset):\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "    test_paths = []\n",
    "    for label in os.listdir(\"preprocessed_clips_2\"):\n",
    "        paths = (lambda x: [f\"preprocessed_clips_2/{x}/{p}\" for p in os.listdir(f\"preprocessed_clips_2/{x}\")])(label)\n",
    "        paths_count = len(paths)\n",
    "        train_ind = math.floor(paths_count * 0.8)\n",
    "        val_ind = train_ind + math.floor(paths_count * 0.1)\n",
    "        test_ind = val_ind + math.floor(paths_count * 0.1)\n",
    "        train_paths.extend(paths[:train_ind])\n",
    "        val_paths.extend(paths[train_ind:val_ind])\n",
    "        test_paths.extend(paths[val_ind:])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, split: str):\n",
    "        self.split = split\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return len(self.train_paths)\n",
    "        elif self.split == \"val\":\n",
    "            return len(self.val_paths)\n",
    "        elif self.split == \"test\":\n",
    "            return len(self.test_paths)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            path = self.train_paths[idx]\n",
    "        elif self.split == \"val\":\n",
    "            path = self.val_paths[idx]\n",
    "        elif self.split == \"test\":\n",
    "            path = self.test_paths[idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split: {self.split}\")\n",
    "        \n",
    "        clip = np.load(path)\n",
    "        \n",
    "        # convert to float and scale to 0-1\n",
    "        clip = clip.astype(np.float32) / 255.0\n",
    "        \n",
    "        # image net mean/std\n",
    "        clip = (clip - self.mean) / self.std\n",
    "        \n",
    "        #reorder to (T,C,H,W)\n",
    "        clip = clip.transpose(0,3,1,2)\n",
    "        \n",
    "        #convert to tensor\n",
    "        clip = torch.from_numpy(clip)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": clip,\n",
    "            \"labels\": torch.tensor(LABEL2ID[path.split(\"/\")[-2]], dtype=torch.long) \n",
    "        }\n",
    "\n",
    "BoxingDataset.train_paths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b171cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BoxingDataset(\n",
    "    split=\"train\",\n",
    ")\n",
    "val_dataset = BoxingDataset(\n",
    "    split=\"val\",\n",
    ")\n",
    "test_dataset = BoxingDataset(\n",
    "    split=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a143d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FACTS used batch_size=4, grad_accum=2, warmup_ratio=0.1, epochs=10\n",
    "# Learning rate is not rendered in the HTML; start with 1e-4 and tune around it.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./facts-boxing-videomae\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=8, \n",
    "    gradient_accumulation_steps=2,  # effective batch size 8\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.05,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"wandb\",  # or \"wandb\"/\"tensorboard\"\n",
    "    dataloader_num_workers=4,        # ADD THIS - use multiple workers\n",
    "    dataloader_pin_memory=True,      # ADD THIS - faster CPU->GPU transfer\n",
    "    dataloader_prefetch_factor=2, \n",
    ")\n",
    "\n",
    "data_collator = VideoDataCollator()\n",
    "\n",
    "train_labels = [LABEL2ID[path.split(\"/\")[-2]] for path in BoxingDataset.train_paths]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(LABEL2ID)),\n",
    "    y=np.array(train_labels)  # Ensure it's a numpy array\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a544dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x72b824759030>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    sample_weights,                                              \n",
    "    len(sample_weights), \n",
    "    replacement=True\n",
    ")\n",
    "sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a0d03a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_counts = defaultdict(int)\n",
    "for i in range(1000):\n",
    "    sampled_idx = random.choices(train_labels, weights=sample_weights, k=1)[0]\n",
    "    sample_counts[sampled_idx] += 1\n",
    "\n",
    "# use matplotlib to plot the distribution\n",
    "\n",
    "plt.bar(sample_counts.keys(), sample_counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c704977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSamplerTrainer(Trainer):\n",
    "    def __init__(self, *args, train_sampler=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_sampler = train_sampler\n",
    "        \n",
    "    def _get_train_sampler(self, train_dataset: Dataset | None = None):\n",
    "        if train_dataset is None:\n",
    "            train_dataset = self.train_dataset\n",
    "            \n",
    "        if train_dataset is None or not has_length(train_dataset):\n",
    "            return None\n",
    "        \n",
    "        if self.train_sampler is not None:\n",
    "            return self.train_sampler\n",
    "        \n",
    "        return super()._get_train_sampler(train_dataset)\n",
    "\n",
    "trainer = BalancedSamplerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_sampler=sampler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5580d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnkosik11\u001b[0m (\u001b[33mnkosik11-hobby\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/wandb/run-20251204_222334-mcaw8c1q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/mcaw8c1q' target=\"_blank\">New Preprocessing - 10 Epochs - Balanced Sampler</a></strong> to <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nkosik11-hobby/facts-classifier/runs/mcaw8c1q' target=\"_blank\">https://wandb.ai/nkosik11-hobby/facts-classifier/runs/mcaw8c1q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7450' max='7450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7450/7450 15:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>F1 Lhhp</th>\n",
       "      <th>Precision Lhhp</th>\n",
       "      <th>Recall Lhhp</th>\n",
       "      <th>F1 Rhhp</th>\n",
       "      <th>Precision Rhhp</th>\n",
       "      <th>Recall Rhhp</th>\n",
       "      <th>F1 Lhmp</th>\n",
       "      <th>Precision Lhmp</th>\n",
       "      <th>Recall Lhmp</th>\n",
       "      <th>F1 Rhmp</th>\n",
       "      <th>Precision Rhmp</th>\n",
       "      <th>Recall Rhmp</th>\n",
       "      <th>F1 Lhblp</th>\n",
       "      <th>Precision Lhblp</th>\n",
       "      <th>Recall Lhblp</th>\n",
       "      <th>F1 Rhblp</th>\n",
       "      <th>Precision Rhblp</th>\n",
       "      <th>Recall Rhblp</th>\n",
       "      <th>F1 Lhbp</th>\n",
       "      <th>Precision Lhbp</th>\n",
       "      <th>Recall Lhbp</th>\n",
       "      <th>F1 Rhbp</th>\n",
       "      <th>Precision Rhbp</th>\n",
       "      <th>Recall Rhbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.080800</td>\n",
       "      <td>2.134721</td>\n",
       "      <td>0.098383</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.190210</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.058900</td>\n",
       "      <td>2.080171</td>\n",
       "      <td>0.088949</td>\n",
       "      <td>0.048094</td>\n",
       "      <td>0.029091</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.215827</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135881</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.036900</td>\n",
       "      <td>2.103424</td>\n",
       "      <td>0.092992</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.090395</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180685</td>\n",
       "      <td>0.102837</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.970600</td>\n",
       "      <td>2.532608</td>\n",
       "      <td>0.043127</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076433</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.066165</td>\n",
       "      <td>0.036304</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.913500</td>\n",
       "      <td>2.967853</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.111940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.786300</td>\n",
       "      <td>2.762403</td>\n",
       "      <td>0.067385</td>\n",
       "      <td>0.054837</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.112821</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.088710</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.109785</td>\n",
       "      <td>0.058974</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.516500</td>\n",
       "      <td>2.789974</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.064257</td>\n",
       "      <td>0.036446</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.184700</td>\n",
       "      <td>2.877296</td>\n",
       "      <td>0.059299</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>0.078176</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.045802</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034188</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>3.199186</td>\n",
       "      <td>0.090296</td>\n",
       "      <td>0.069801</td>\n",
       "      <td>0.102389</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.057252</td>\n",
       "      <td>0.167832</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189349</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>3.105659</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.091205</td>\n",
       "      <td>0.172702</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.286624</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.335821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>3.160191</td>\n",
       "      <td>0.180593</td>\n",
       "      <td>0.123141</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.183206</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.265487</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.040293</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>3.632272</td>\n",
       "      <td>0.159030</td>\n",
       "      <td>0.115321</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>0.273038</td>\n",
       "      <td>0.251572</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.289600</td>\n",
       "      <td>3.882301</td>\n",
       "      <td>0.168464</td>\n",
       "      <td>0.111213</td>\n",
       "      <td>0.220513</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.264317</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>4.096323</td>\n",
       "      <td>0.168464</td>\n",
       "      <td>0.106502</td>\n",
       "      <td>0.268793</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>0.288401</td>\n",
       "      <td>0.248649</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7450, training_loss=1.2301131463530879, metrics={'train_runtime': 935.1773, 'train_samples_per_second': 63.688, 'train_steps_per_second': 7.966, 'total_flos': 7.421957814019424e+19, 'train_loss': 1.2301131463530879, 'epoch': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_NAME\"] = \"New Preprocessing - 10 Epochs - Balanced Sampler\"\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "751fcb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 2.718714475631714, 'eval_accuracy': 0.23435419440745672, 'eval_macro_f1': 0.15463188837138075, 'eval_f1_LHHP': 0.32916666666666666, 'eval_precision_LHHP': 0.3640552995391705, 'eval_recall_LHHP': 0.30038022813688214, 'eval_f1_RHHP': 0.1935483870967742, 'eval_precision_RHHP': 0.21428571428571427, 'eval_recall_RHHP': 0.17647058823529413, 'eval_f1_LHMP': 0.28823529411764703, 'eval_precision_LHMP': 0.22790697674418606, 'eval_recall_LHMP': 0.392, 'eval_f1_RHMP': 0.174496644295302, 'eval_precision_RHMP': 0.18309859154929578, 'eval_recall_RHMP': 0.16666666666666666, 'eval_f1_LHBlP': 0.08139534883720931, 'eval_precision_LHBlP': 0.0625, 'eval_recall_LHBlP': 0.11666666666666667, 'eval_f1_RHBlP': 0.0, 'eval_precision_RHBlP': 0.0, 'eval_recall_RHBlP': 0.0, 'eval_f1_LHBP': 0.0, 'eval_precision_LHBP': 0.0, 'eval_recall_LHBP': 0.0, 'eval_f1_RHBP': 0.1702127659574468, 'eval_precision_RHBP': 0.23529411764705882, 'eval_recall_RHBP': 0.13333333333333333, 'eval_runtime': 7.2123, 'eval_samples_per_second': 104.128, 'eval_steps_per_second': 13.033, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test split\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c80814",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
